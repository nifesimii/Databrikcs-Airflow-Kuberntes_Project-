{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ba23f7-a9de-4d0f-86b1-4f4db26d141b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_posts_df = spark.table(\"default.raw_posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83b7eea-3d06-4fe9-bd1d-c00752678d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_posts_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffccd992-4935-405c-a7bb-e0a2909a3e6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declarative Transformations"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def split_tag_into_array(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            \"TagsArray\",\n",
    "            F.filter(F.split(F.col(\"tags\"),r'\\|'), lambda x: x != \"\"))\n",
    "        .drop(\"tags\")\n",
    "    )\n",
    "\n",
    "def rename_columns(df: DataFrame) -> DataFrame:\n",
    "    return df.withColumnRenamed(\"Id\", \"PostId\")\n",
    "\n",
    "def map_post_type(df: DataFrame) -> DataFrame:\n",
    "    map_data = spark.createDataFrame(\n",
    "        [\n",
    "            (1, \"Question\"),\n",
    "            (2, \"Answer\"),\n",
    "            (3, \"Orphaned tag wiki\"),\n",
    "            (4, \"Tag wiki excerpt\"),\n",
    "            (5, \"Tag wiki\"),\n",
    "            (6, \"Moderator nomination\"),\n",
    "            (7, \"Wiki placeholder\"),\n",
    "            (8, \"Privilege wiki\"),\n",
    "            (9, \"Article\"),\n",
    "            (10, \"HelpArticle\"),\n",
    "            (12, \"Collection\"),\n",
    "            (13, \"ModeratorQuestionnaireResponse\"),\n",
    "            (14, \"Announcement\"),\n",
    "            (15, \"CollectiveDiscussion\"),\n",
    "            (17, \"CollectiveCollection\")\n",
    "        ],\n",
    "        [\"PostTypeId\", \"PostTypeName\"]\n",
    "    )\n",
    "    return df.join(\n",
    "        F.broadcast(map_data),\n",
    "        on=\"PostTypeId\",\n",
    "        how=\"left\"\n",
    "    ).drop(map_data[\"PostTypeId\"])\n",
    "\n",
    "stg_post_df = (\n",
    "    raw_posts_df\n",
    "    .transform(split_tag_into_array)\n",
    "    .transform(rename_columns)\n",
    "    .transform(map_post_type)\n",
    ")\n",
    "\n",
    "display(stg_post_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81af782-c8f6-4a46-b4ae-744b65c6b353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(stg_post_df.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0c95d1-b9df-4233-9562-c7091d78ee21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Incremental Upsert"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def incremental_upsert(dest_table:str, df:DataFrame, uniqueKey:str, updated_at:str, full_refresh=False):\n",
    "    \"\"\"\n",
    "    Performs incremental upsert using updated_at as the cursor_value with unique_key \n",
    "    Doesnt support deleted, very minimal\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(dest_table) or full_refresh:\n",
    "        (\n",
    "            df\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(dest_table)\n",
    "        )\n",
    "    else:\n",
    "        last_max = (\n",
    "            spark.table(dest_table)\n",
    "            .agg(F.max(updated_at).alias(\"max_ts\"))\n",
    "            .collect()[0][\"max_ts\"]                 \n",
    "        )\n",
    "\n",
    "        incr_df = df.filter(F.col(updated_at) > last_max)\n",
    "\n",
    "        if incr_df.head(1):\n",
    "            delta_table = DeltaTable.forName(spark,dest_table)\n",
    "            (\n",
    "            delta_table.alias(\"t\").merge(\n",
    "                source=incr_df.alias(\"s\"),\n",
    "                condition=f\"s.{unique_key} = t.{unique_key}\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "            )\n",
    " \n",
    "\n",
    "dest_table = \"default.stg_posts\"\n",
    "unique_key = \"PostId\"\n",
    "updated_at = \"CreationDate\"\n",
    "incremental_upsert(dest_table,stg_post_df,unique_key,updated_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6150023e-9507-4f18-90e5-a7df5dd342f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize Spark Writes"
    }
   },
   "outputs": [],
   "source": [
    "# spark.table(dest_table).rdd.getNumPartitions()  # RDD operations are not supported on shared clusters, consider alternative methods such as spark.table(dest_table).rdd.getNumPartitions() if supported, or use DataFrame methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49dff978-943f-4a27-b345-610253db1ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incremental_upsert(dest_table,stg_post_df.repartition(4),\"PostId\",\"CreationDate\",full_refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f921ee-534a-47b4-af6b-a07379a3ca1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8f9458-6df0-46bc-8005-f2f6f3e99a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(dest_table).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b3f1f3-f8aa-43c3-acbf-0c29b118c535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_posts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
